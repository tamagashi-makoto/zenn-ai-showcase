import os  # â† æœ€å„ªå…ˆã§ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã™ã‚‹ãŸã‚ã«å…ˆã«èª­ã¿è¾¼ã‚€

# ====== APIã‚­ãƒ¼ã‚’â€œimport ollamaâ€ã®å‰ã«è¨­å®šã™ã‚‹ï¼ˆé‡è¦ï¼‰ ======
DEFAULT_OLLAMA_API_KEY = "PUT_YOUR_OLLAMA_API"
_env_key = (os.getenv("OLLAMA_API_KEY") or DEFAULT_OLLAMA_API_KEY).strip()
# éASCIIå¯¾ç­–ï¼ˆå¿µã®ãŸã‚ï¼‰
try:
    if any(ord(ch) > 127 for ch in _env_key):
        _env_key = DEFAULT_OLLAMA_API_KEY
except Exception:
    pass
os.environ["OLLAMA_API_KEY"] = _env_key  # â† ã“ã“ã§ç¢ºå®Ÿã«è¨­å®šã—ã¦ã‹ã‚‰ ollama ã‚’ import

import streamlit as st
import textwrap
import time
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import requests
import ollama
import urllib.parse
import xml.etree.ElementTree as ET
import html

# ==============================
# ãƒšãƒ¼ã‚¸è¨­å®šã¨ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆUIã®åŸºæœ¬ã¯ãã®ã¾ã¾ï¼‰
# ==============================
st.set_page_config(
    page_title="AI News Daily",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main-header {
        font-size: 3rem; font-weight: 700;
        background: linear-gradient(120deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text; -webkit-text-fill-color: transparent;
        text-align: center; padding: 1rem 0;
    }
    .subtitle { text-align: center; color: #6c757d; font-size: 1.2rem; margin-bottom: 2rem; }
    .stButton>button {
        width: 100%; background: linear-gradient(120deg, #667eea 0%, #764ba2 100%);
        color: white; font-weight: 600; border-radius: 8px;
        padding: 0.75rem 1.5rem; border: none; margin-top: 0.5rem;
    }
    .section-title {
        font-size: 1.4rem; font-weight: 700; margin: 1rem 0 0.5rem 0;
    }
    .source-card {
        padding: 0.75rem 1rem; border: 1px solid #eaeaea; border-radius: 8px;
        background: #fafafa; margin-bottom: 0.75rem;
    }
    .footer {
        color: #6c757d; font-size: 0.9rem; text-align: center; margin-top: 2rem;
    }
    a { text-decoration: none; font-size: 0.9rem; }
    /* å…¥åŠ›çª“ã‚’å¤§ããè¦‹ã‚„ã™ã */
    .big-chat textarea {
        min-height: 160px !important;
        font-size: 1.05rem !important;
        line-height: 1.6 !important;
    }
</style>
""", unsafe_allow_html=True)

# ==============================
# æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå…¬å¼ä»•æ§˜æº–æ‹ ï¼‰
# 1) REST:   POST https://ollama.com/api/web_search  Authorization: Bearer <OLLAMA_API_KEY>
#    body:   {"query": "..."}
# 2) Python: ollama.web_search(query)
# ==============================
class UniversalSearchClient:
    def __init__(self, api_key: Optional[str] = None, timeout: float = 30.0):
        self.api_key = (api_key or os.getenv("OLLAMA_API_KEY", "")).strip()
        self.timeout = timeout

        # å…¬å¼RESTã®å›ºå®šã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        self.fixed_search_url = (os.getenv("OLLAMA_WEB_SEARCH_URL", "")).strip() or "https://ollama.com/api/web_search"

        # HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆAuthorization: Bearer ã®ã¿ï¼‰
        self.session = requests.Session()
        if self.api_key:
            self.session.headers.update({
                "Authorization": f"Bearer {self.api_key}",
            })
        self.session.headers.update({
            "Accept": "application/json",
            "User-Agent": "Mozilla/5.0 (compatible; AI-News-Dashboard/1.0)",
        })

    # -------- 1) å…¬å¼RESTã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆå…ˆã«å©ãï¼šä¸Šé™ã‚¨ãƒ©ãƒ¼ã‚’å³æ¤œå‡ºï¼‰ --------
    def _try_ollama_http(self, query: str, max_results: int) -> Optional[List[Dict]]:
        try:
            resp = self.session.post(
                self.fixed_search_url,
                json={"query": query},
                timeout=self.timeout
            )
            if resp.status_code == 200:
                data = resp.json()
                items = self._normalize_search(data)[:max_results]
                if not items:
                    st.info("[REST] 200 ã ãŒ results ãŒç©ºã€‚ã‚¯ã‚¨ãƒªå†…å®¹/å¯¾è±¡æœŸé–“ã®å¯èƒ½æ€§ã€‚")
                else:
                    st.info(f"[REST] web_search OK: {len(items)} ä»¶")
                return items if items else None
            else:
                snippet = (resp.text or "")[:400].replace("\n"," ")
                st.info(f"[REST] {resp.status_code} {self.fixed_search_url}  body={snippet}")
                # 402/401/403/429 ã¯ã“ã“ã§æ‰“ã¡åˆ‡ã‚‹ï¼ˆSDKã§äºŒé‡ã«å©ã‹ãªã„ï¼‰
                if resp.status_code in (401, 402, 403, 429):
                    return None
        except Exception as e:
            st.info(f"[REST] ä¾‹å¤–: {e}")
        return None

    # -------- 2) Ollama Python SDK --------
    def _try_ollama_sdk(self, query: str, max_results: int) -> Optional[List[Dict]]:
        try:
            res = ollama.web_search(query)
            items = self._normalize_search(res)[:max_results]
            if not items:
                st.info("[SDK] web_search ã¯æˆåŠŸã—ã¾ã—ãŸãŒã€results ãŒç©ºã§ã—ãŸã€‚")
            else:
                st.info(f"[SDK] web_search OK: {len(items)} ä»¶")
            return items if items else None
        except Exception as e:
            st.info(f"[SDK] web_search ä¾‹å¤–: {e}")
            return None

    # -------- å…¬é–‹ï¼šæ¤œç´¢ï¼ˆRESTâ†’SDK ã®é †ã§è©¦ã™ï¼‰ --------
    def search(self, query: str, max_results: int = 20) -> List[Dict]:
        res = self._try_ollama_http(query, max_results)
        if res:
            return res
        res = self._try_ollama_sdk(query, max_results)
        if res:
            return res
        st.info("SDK/REST ã„ãšã‚Œã‚‚çµæœã‚¼ãƒ­ã§ã—ãŸã€‚APIã‚­ãƒ¼ãƒ»ãƒ¬ãƒ¼ãƒˆãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        return []

    # -------- æ­£è¦åŒ–ï¼ˆ{"results":[...]} æƒ³å®šï¼‰ --------
    @staticmethod
    def _normalize_search(data) -> List[Dict]:
        if isinstance(data, list):
            items = data
        elif isinstance(data, dict):
            if isinstance(data.get("results"), list):
                items = data["results"]
            else:
                items = next((v for v in data.values() if isinstance(v, list)), [])
        else:
            items = []
        norm = []
        for it in items:
            if not isinstance(it, dict):
                continue
            url = it.get("url") or it.get("link") or ""
            title = it.get("title") or it.get("name") or ""
            content = it.get("content") or it.get("snippet") or it.get("text") or ""
            if url:
                norm.append({"url": url, "title": title, "content": content})
        return norm

# ==============================
# è¨­å®šï¼ˆAPIã‚­ãƒ¼ï¼‰
# ==============================
OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY", "").strip()  # ã™ã§ã«å…ˆé ­ã§è¨­å®šæ¸ˆã¿

# æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆç’°å¢ƒã«å¿œã˜ã¦è‡ªå‹•é¸æŠï¼‰
search_client = UniversalSearchClient(api_key=OLLAMA_API_KEY)

# ãƒ­ãƒ¼ã‚«ãƒ« LLMï¼ˆgemma3:4bï¼‰
try:
    # èªè¨¼ä¸è¦ã®ãƒ­ãƒ¼ã‚«ãƒ«æ¨è«–ç”¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    ollama_local_client = ollama.Client()
except Exception as e:
    st.error(f"Ollama ãƒ­ãƒ¼ã‚«ãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

# ==============================
# æ¤œç´¢ï¼ˆfetchã¯ã—ãªã„ï¼‰
# ==============================
@st.cache_data(ttl=1800)
def web_search_and_fetch(query: str, max_results: int = 20) -> List[Dict]:
    """
    fetchã¯è¡Œã‚ãšã€æ¤œç´¢çµæœï¼ˆtitle/url/snippetï¼‰ã ã‘ã‚’è¿”ã™ã€‚
    å…¬å¼ã® web_search çµæœï¼ˆtitle/url/contentï¼‰ã‚’å‰æã«æ•´å½¢ã€‚
    """
    results = search_client.search(query, max_results=max_results)

    status = st.status("æ¤œç´¢çµæœã‚’æ•´å½¢ä¸­...", expanded=False)
    out: List[Dict] = []
    seen = set()
    for i, r in enumerate(results, 1):
        url = r.get("url")
        if not url or url in seen:
            continue
        seen.add(url)
        title = r.get("title") or ""
        content = r.get("content") or ""
        out.append({"title": title, "url": url, "content": content})
        time.sleep(0.01)
    status.update(label=f"âœ… {len(out)}ä»¶ã®æ¤œç´¢çµæœã‚’å–å¾—", state="complete")
    return out

# ==============================
# ãƒŠãƒ©ãƒ†ã‚£ãƒ–ç”Ÿæˆï¼ˆgemma3:4bï¼‰
# ==============================
def analyze_ai_news_narrative(sources: List[Dict], selected_date: Optional[str]) -> str:
    def clip(txt: str, max_chars=8000):
        return txt[:max_chars] if isinstance(txt, str) else ""

    packed_sources = []
    for i, s in enumerate(sources, 1):
        packed_sources.append(
            f"[ã‚½ãƒ¼ã‚¹{i}]\nã‚¿ã‚¤ãƒˆãƒ«: {s.get('title','')}\nURL: {s.get('url','')}\nå†…å®¹:\n{clip(s.get('content',''))}\n"
        )

    system = textwrap.dedent("""
    ã‚ãªãŸã¯çµŒé¨“è±Šå¯ŒãªAIæŠ€è¡“ã‚¸ãƒ£ãƒ¼ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
    æä¾›ã•ã‚ŒãŸæƒ…å ±æºã‚’æ·±ãåˆ†æã—ã€èª­è€…ã«åˆ†ã‹ã‚Šã‚„ã™ãé­…åŠ›çš„ãªè¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    
    é‡è¦ãªè¦ä»¶:
    - å„AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦ã€ç‹¬ç«‹ã—ãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹
    - å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¯æ˜ç¢ºãªã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ã¨è©³ç´°ãªã€Œå†…å®¹ã€ã‚’å«ã‚ã‚‹
    - ç®‡æ¡æ›¸ãã¯ä½¿ç”¨ã›ãšã€æµã‚Œã‚‹ã‚ˆã†ãªãƒŠãƒ©ãƒ†ã‚£ãƒ–ï¼ˆç‰©èªçš„ï¼‰ãªæ–‡ç« ã§æ›¸ã
    - æŠ€è¡“çš„ãªå†…å®¹ã‚’ä¸€èˆ¬èª­è€…ã«ã‚‚ç†è§£ã§ãã‚‹ã‚ˆã†å™›ã¿ç •ã„ã¦èª¬æ˜ã™ã‚‹
    - å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®èƒŒæ™¯ã€æ„ç¾©ã€å½±éŸ¿ã‚’æ·±ãæ˜ã‚Šä¸‹ã’ã‚‹
    - å°‚é–€ç”¨èªã«ã¯ç°¡å˜ãªèª¬æ˜ã‚’æ·»ãˆã‚‹
    - å¼•ç”¨å…ƒã¯æ–‡ä¸­ã«è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã‚€ï¼ˆä¾‹: ã€Œã€œã«ã‚ˆã‚‹ã¨[1]ã€ï¼‰
    - æœ€å¾Œã«ã€Œå¼•ç”¨å…ƒä¸€è¦§ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ã€[ç•ªå·] URL ã®å½¢å¼ã§åˆ—æŒ™
    
    æ–‡ç« ã‚¹ã‚¿ã‚¤ãƒ«:
    - èª­è€…ã‚’å¼•ãè¾¼ã‚€å°å…¥æ–‡ã‹ã‚‰å§‹ã‚ã‚‹
    - ã€Œã§ã‚ã‚‹ã€èª¿ã®è½ã¡ç€ã„ãŸæ–‡ä½“
    - å…·ä½“ä¾‹ã‚„æ¯”å–©ã‚’ç”¨ã„ã¦ç†è§£ã‚’ä¿ƒé€²
    - å„æ®µè½ã¯3-5æ–‡ç¨‹åº¦ã§æ§‹æˆ
    - ãƒ‹ãƒ¥ãƒ¼ã‚¹é–“ã®é–¢é€£æ€§ãŒã‚ã‚Œã°è¨€åŠã™ã‚‹
    - æ—¥æœ¬èªã§å‡ºåŠ›
    """).strip()

    separator = "\n\n" + "="*50 + "\n\n"
    sources_text = separator.join(packed_sources)
    today = selected_date or datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

    prompt = f"""
æœ¬æ—¥ï¼ˆ{today}ï¼‰ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä»¥ä¸‹ã®æƒ…å ±æºã‹ã‚‰æ·±ãåˆ†æã—ã¦æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ã€æƒ…å ±æºã€‘
{sources_text}

ã€å‡ºåŠ›å½¢å¼ã€‘
# æœ¬æ—¥ã®AIæŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ - {today}

[å°å…¥éƒ¨: ä»Šæ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹å…¨ä½“ã®æ¦‚è¦ã‚’2-3æ®µè½ã§]

## [ãƒ‹ãƒ¥ãƒ¼ã‚¹1ã®ã‚¿ã‚¤ãƒˆãƒ«]

[ãƒŠãƒ©ãƒ†ã‚£ãƒ–å½¢å¼ã®è©³ç´°ãªå†…å®¹ã€‚3-5æ®µè½ç¨‹åº¦]

## [ãƒ‹ãƒ¥ãƒ¼ã‚¹2ã®ã‚¿ã‚¤ãƒˆãƒ«]

[ãƒŠãƒ©ãƒ†ã‚£ãƒ–å½¢å¼ã®è©³ç´°ãªå†…å®¹ã€‚3-5æ®µè½ç¨‹åº¦]

[ä»¥é™ã€é‡è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã”ã¨ã«åŒæ§˜ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ]

---

## å¼•ç”¨å…ƒä¸€è¦§
[1] [URL]
[2] [URL]
...
"""
    full_prompt = f"{system}\n\n{prompt}"

    try:
        response = ollama_local_client.chat(
            model='gemma3:4b',
            messages=[{'role': 'user', 'content': full_prompt}],
            options={'temperature': 0.8, 'num_predict': 8000}
        )
        if isinstance(response, dict):
            content = response.get('message', {}).get('content', '')
        else:
            content = getattr(getattr(response, 'message', None), 'content', '') or ''
        return content or "(å¿œç­”ãªã—)"
    except Exception as e:
        error_msg = f"Error during analysis: {str(e)}\n\n"
        error_msg += "Please ensure Ollama is running locally and gemma3:4b model is installed.\n"
        error_msg += "Run: ollama pull gemma3:4b"
        return error_msg

# ==============================
# UIï¼ˆå…¥åŠ›ã‚¯ã‚¨ãƒªã ã‘ãƒ¡ã‚¤ãƒ³å´ã®â€œã§ã£ã‹ã„ãƒãƒ£ãƒƒãƒˆçª“â€ã«ç§»å‹•ï¼‰
# ==============================
st.markdown('<div class="main-header">AI News Daily</div>', unsafe_allow_html=True)
st.markdown('<div class="subtitle">æœ€æ–°ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã€Ollamaã§ç‰©èªèª¿ã«æ·±æ˜ã‚Šè§£èª¬</div>', unsafe_allow_html=True)

# --- è¦‹å‡ºã—ç›´ä¸‹ã«å¤§ããªå…¥åŠ›çª“ã‚’é…ç½®ï¼ˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å¤‰æ›´ã—ãªã„ï¼‰ ---
default_query = f"AI news artificial intelligence latest {datetime.now().strftime('%Y-%m-%d')}"
if 'current_query' not in st.session_state:
    st.session_state.current_query = ""

with st.container():
    # å¤§ããè¦‹ã‚„ã™ã„å…¥åŠ›æ¬„ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã¯ãªããƒ¡ã‚¤ãƒ³ï¼‰
    st.markdown('<div class="big-chat">', unsafe_allow_html=True)
    main_query = st.text_area(
        label="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›",
        value=st.session_state.current_query or default_query,
        placeholder="ä¾‹: latest AI research breakthroughs today",
        height=170,
    )
    st.markdown('</div>', unsafe_allow_html=True)
    # å…¥åŠ›ã®å€¤ã ã‘æ›´æ–°ï¼ˆå®Ÿè¡Œã¯å¾“æ¥ã©ãŠã‚Šã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ã§è¡Œã†ï¼‰
    st.session_state.current_query = (main_query or "").strip()

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆã‚¯ã‚¨ãƒªä»¥å¤–ã¯ãã®ã¾ã¾ï¼‰
with st.sidebar:
    st.header("æ¤œç´¢è¨­å®š")
    # â€» ã‚¯ã‚¨ãƒªå…¥åŠ›ã¯ã“ã“ã«ã¯ç½®ã‹ãªã„ï¼ˆå ´æ‰€ç§»å‹•ã®ã¿ï¼‰
    max_results = st.slider("æ¤œç´¢ä»¶æ•°ï¼ˆä¸Šé™ï¼‰", min_value=5, max_value=50, value=20, step=5)
    selected_date = st.date_input("è¨˜äº‹ã®æ—¥ä»˜ï¼ˆè¦‹å‡ºã—ç”¨ï¼‰", value=datetime.now().date(), format="YYYY/MM/DD")
    selected_date = selected_date.strftime("%Yå¹´%mæœˆ%dæ—¥") if selected_date else None
    st.markdown("---")
    run_btn = st.button("ğŸ” ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ â†’ ğŸ§  ãƒŠãƒ©ãƒ†ã‚£ãƒ–ç”Ÿæˆ", use_container_width=True)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ï¼ˆãã®ã¾ã¾ï¼‰
if 'sources' not in st.session_state: st.session_state.sources = []
if 'article' not in st.session_state: st.session_state.article = ""
if 'current_date' not in st.session_state: st.session_state.current_date = None

# æ—¢å­˜ã®ã€Œå®Ÿè¡Œãƒœã‚¿ãƒ³ã€å‹•ä½œã¯ç¶­æŒ
if run_btn:
    st.session_state.article = ""
    st.session_state.sources = []
    st.session_state.current_date = selected_date
    effective_query = st.session_state.current_query or default_query

    with st.spinner("ğŸ” Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­..."):
        sources = web_search_and_fetch(effective_query, max_results=max_results)
        st.session_state.sources = sources

    if sources:
        st.success(f"âœ… {len(sources)}ä»¶ã®æ¤œç´¢çµæœã‚’å–å¾—ã—ã¾ã—ãŸ")
        with st.spinner("ğŸ¤– ãƒ­ãƒ¼ã‚«ãƒ«ã®gemma3:4bã§åˆ†æãƒ»è¦ç´„ã‚’ç”Ÿæˆä¸­..."):
            article = analyze_ai_news_narrative(sources, selected_date)
            if article:
                st.session_state.article = article
                st.balloons()
    else:
        st.warning("åˆ†æå¯¾è±¡ã¨ãªã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# å‡ºåŠ›è¡¨ç¤ºï¼ˆãã®ã¾ã¾ï¼‰
if st.session_state.article:
    st.markdown("---")
    st.markdown(st.session_state.article, unsafe_allow_html=True)
    with st.expander("å‚ç…§ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ä¸€è¦§ã‚’è¦‹ã‚‹"):
        for i, s in enumerate(st.session_state.sources, 1):
            st.markdown(f"**{i}. {s.get('title','(no title)')}**")
            st.markdown(f"<{s.get('url','')}>", unsafe_allow_html=True)

st.markdown('<div class="footer">Powered by Ollama Web Search + gemma3:4b</div>', unsafe_allow_html=True)