# ai_news_dashboard.py
import streamlit as st
import os
import textwrap
import time
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import requests
import ollama
import urllib.parse
import xml.etree.ElementTree as ET
import html

# ==============================
# ãƒšãƒ¼ã‚¸è¨­å®šã¨ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆUIã®åŸºæœ¬ã¯ãã®ã¾ã¾ï¼‰
# ==============================
st.set_page_config(
    page_title="AI News Daily",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main-header {
        font-size: 3rem; font-weight: 700;
        background: linear-gradient(120deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text; -webkit-text-fill-color: transparent;
        text-align: center; padding: 1rem 0;
    }
    .subtitle { text-align: center; color: #6c757d; font-size: 1.2rem; margin-bottom: 2rem; }
    .stButton>button {
        width: 100%; background: linear-gradient(120deg, #667eea 0%, #764ba2 100%);
        color: white; font-weight: 600; border-radius: 8px;
        padding: 0.75rem 1.5rem; border: none; margin-top: 0.5rem;
    }
    .section-title {
        font-size: 1.4rem; font-weight: 700; margin: 1rem 0 0.5rem 0;
    }
    .source-card {
        padding: 0.75rem 1rem; border: 1px solid #eaeaea; border-radius: 8px;
        background: #fafafa; margin-bottom: 0.75rem;
    }
    .footer {
        color: #6c757d; font-size: 0.9rem; text-align: center; margin-top: 2rem;
    }
    a { text-decoration: none; font-size: 0.9rem; }
    /* å…¥åŠ›çª“ã‚’å¤§ããè¦‹ã‚„ã™ã */
    .big-chat textarea {
        min-height: 160px !important;
        font-size: 1.05rem !important;
        line-height: 1.6 !important;
    }
</style>
""", unsafe_allow_html=True)

# ==============================
# æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆfetchã›ãšã€æ¤œç´¢çµæœã ã‘è¿”ã™ï¼‰
# 1) ollama.Client.web_search ãŒä½¿ãˆã‚Œã°æœ€å„ªå…ˆ
# 2) OLLAMA_WEB_SEARCH_URL / OLLAMA_WEB_BASE_URL çµŒç”±ã®HTTPï¼ˆå¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã«ã—ãªã„ï¼‰
# 3) ç„¡éµãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šGoogle News RSS / Bing News RSSï¼ˆbs4/docduckä¸ä½¿ç”¨ï¼‰
# 4) ã•ã‚‰ã«éµãŒã‚ã‚Œã° Serper / Brave / Bing API ã‚‚åˆ©ç”¨å¯èƒ½ï¼ˆä»»æ„ï¼‰
# ==============================
class UniversalSearchClient:
    def __init__(self, api_key: Optional[str] = None, timeout: float = 30.0):
        self.api_key = (api_key or os.getenv("OLLAMA_API_KEY", "")).strip()
        self.timeout = timeout

        # ---- Ollama SDKï¼ˆweb_search ãŒã‚ã‚Œã°ä½¿ã†ï¼‰
        self._sdk_client = None
        self._has_sdk_search = False
        try:
            sdk_host = os.getenv("OLLAMA_HOST", "") or os.getenv("OLLAMA_WEB_BASE_URL", "")
            headers = {"Authorization": f"Bearer {self.api_key}"} if self.api_key else None
            self._sdk_client = ollama.Client(host=sdk_host.strip() or None, headers=headers)
            self._has_sdk_search = hasattr(self._sdk_client, "web_search")
        except Exception:
            self._sdk_client = None
            self._has_sdk_search = False

        # ---- HTTPç›´å©ãç”¨ï¼ˆOllamaç³»ï¼‰
        self.session = requests.Session()
        if self.api_key:
            self.session.headers.update({
                "Authorization": f"Bearer {self.api_key}",
                "Ollama-Api-Key": self.api_key,
                "X-API-Key": self.api_key,
            })
        self.session.headers.update({
            "Accept": "application/json,text/plain,*/*",
            "User-Agent": "Mozilla/5.0 (compatible; AI-News-Dashboard/1.0)",
        })

        self.fixed_search_url = (os.getenv("OLLAMA_WEB_SEARCH_URL", "")).strip() or None
        base_env = (os.getenv("OLLAMA_WEB_BASE_URL", "") or os.getenv("OLLAMA_HOST", "")).strip()
        self.base_candidates: List[str] = []
        if base_env:
            self.base_candidates.append(base_env.rstrip("/"))
        self.base_candidates += [
            "https://api.ollama.com",
            "https://api.ollama.ai",
            "https://cloud.ollama.com",
            "http://localhost:11434",
        ]
        self.search_paths = [
            "/web/search", "/api/web/search",
            "/web_search", "/api/web_search",
            "/search", "/api/search",
            "/v1/web/search", "/api/v1/web/search",
            "/v1/search", "/api/v1/search",
        ]

        # ---- ä»£æ›¿ã®å¤–éƒ¨æ¤œç´¢APIï¼ˆä»»æ„ï¼‰
        self.serper_key = os.getenv("SERPER_API_KEY", "").strip()
        self.brave_key  = os.getenv("BRAVE_API_KEY", "").strip()
        self.bing_key   = os.getenv("BING_SUBSCRIPTION_KEY", "").strip()

    # -------- 1) Ollama SDK --------
    def _try_ollama_sdk(self, query: str, max_results: int) -> Optional[List[Dict]]:
        if not (self._sdk_client and self._has_sdk_search):
            return None
        try:
            res = self._sdk_client.web_search(query, max_results=max_results)
            # normalize
            if hasattr(res, "results"):
                out = []
                for it in res.results:
                    url = getattr(it, "url", "") or ""
                    title = getattr(it, "title", "") or ""
                    content = getattr(it, "content", "") or ""
                    if url:
                        out.append({"url": url, "title": title, "content": content})
                return out
            elif isinstance(res, dict) or isinstance(res, list):
                return self._normalize_search(res)
        except Exception:
            return None
        return None

    # -------- 2) Ollama HTTPï¼ˆå¤±æ•—ã—ã¦ã‚‚ä¾‹å¤–ã¯æŠ•ã’ãªã„ï¼‰ --------
    def _try_ollama_http(self, query: str, max_results: int) -> Optional[List[Dict]]:
        # å›ºå®šURL
        if self.fixed_search_url:
            r = self._http_search_once(self.fixed_search_url, query, max_results)
            if r:
                return r
        # base x path
        for b in self.base_candidates:
            for p in self.search_paths:
                url = f"{b.rstrip('/')}{p}"
                r = self._http_search_once(url, query, max_results)
                if r:
                    return r
        return None

    def _http_search_once(self, url: str, query: str, max_results: int) -> Optional[List[Dict]]:
        # GET
        for params in (
            {"q": query, "k": max_results},
            {"query": query, "k": max_results},
            {"q": query, "limit": max_results},
            {"query": query, "max_results": max_results},
        ):
            try:
                resp = self.session.get(url, params=params, timeout=self.timeout)
                if resp.status_code == 200:
                    try:
                        data = resp.json()
                    except Exception:
                        data = {}
                    items = self._normalize_search(data)
                    if items:
                        return items
            except Exception:
                pass
        # POST
        for payload in (
            {"q": query, "k": max_results},
            {"query": query, "k": max_results},
            {"q": query, "limit": max_results},
            {"query": query, "max_results": max_results},
            {"q": query, "limit": max_results, "max_results": max_results},
        ):
            try:
                resp = self.session.post(url, json=payload, timeout=self.timeout)
                if resp.status_code == 200:
                    try:
                        data = resp.json()
                    except Exception:
                        data = {}
                    items = self._normalize_search(data)
                    if items:
                        return items
            except Exception:
                pass
        return None

    # -------- 3) ç„¡éµRSSï¼ˆç¢ºå®Ÿã«çµæœã‚’è¿”ã™ï¼‰ --------
    def _try_google_news_rss(self, query: str, max_results: int) -> Optional[List[Dict]]:
        q = f"{query} when:7d"
        q_enc = urllib.parse.quote_plus(q)
        url = f"https://news.google.com/rss/search?q={q_enc}&hl=en-US&gl=US&ceid=US:en"
        try:
            r = requests.get(url, timeout=self.timeout, headers={"User-Agent": "Mozilla/5.0"})
            if r.status_code != 200 or not r.text:
                return None
            return self._parse_rss_items(r.text, max_results)
        except Exception:
            return None

    def _try_bing_news_rss(self, query: str, max_results: int) -> Optional[List[Dict]]:
        q_enc = urllib.parse.quote_plus(query)
        url = f"https://www.bing.com/news/search?q={q_enc}&format=rss"
        try:
            r = requests.get(url, timeout=self.timeout, headers={"User-Agent": "Mozilla/5.0"})
            if r.status_code != 200 or not r.text:
                return None
            return self._parse_rss_items(r.text, max_results)
        except Exception:
            return None

    @staticmethod
    def _parse_rss_items(xml_text: str, max_results: int) -> List[Dict]:
        items: List[Dict] = []
        try:
            root = ET.fromstring(xml_text)
        except Exception:
            return items
        for it in root.findall(".//item"):
            title = html.unescape(it.findtext("title") or "")
            link = (it.findtext("link") or "").strip()
            desc = html.unescape(it.findtext("description") or "")
            if link:
                items.append({"title": title, "url": link, "content": desc})
            if len(items) >= max_results:
                break
        return items

    # -------- 4) è¿½åŠ ã®å¤–éƒ¨APIï¼ˆä»»æ„ï¼‰ --------
    def _try_serper(self, query: str, max_results: int) -> Optional[List[Dict]]:
        if not self.serper_key:
            return None
        try:
            url = "https://google.serper.dev/search"
            headers = {"X-API-KEY": self.serper_key, "Content-Type": "application/json"}
            payload = {"q": query, "num": max_results}
            r = requests.post(url, headers=headers, json=payload, timeout=self.timeout)
            if r.status_code != 200:
                return None
            data = r.json()
            items = []
            for it in data.get("organic", [])[:max_results]:
                items.append({
                    "title": it.get("title") or "",
                    "url": it.get("link") or "",
                    "content": it.get("snippet") or "",
                })
            if len(items) < max_results:
                news_url = "https://google.serper.dev/news"
                r2 = requests.post(news_url, headers=headers, json=payload, timeout=self.timeout)
                if r2.status_code == 200:
                    d2 = r2.json()
                    for it in d2.get("news", []):
                        if len(items) >= max_results:
                            break
                        items.append({
                            "title": it.get("title") or "",
                            "url": it.get("link") or "",
                            "content": it.get("snippet") or "",
                        })
            return [x for x in items if x["url"]]
        except Exception:
            return None

    def _try_brave(self, query: str, max_results: int) -> Optional[List[Dict]]:
        if not self.brave_key:
            return None
        try:
            url = "https://api.search.brave.com/res/v1/web/search"
            headers = {"X-Subscription-Token": self.brave_key, "Accept": "application/json"}
            params = {"q": query, "count": max_results}
            r = requests.get(url, headers=headers, params=params, timeout=self.timeout)
            if r.status_code != 200:
                return None
            data = r.json()
            items = []
            for it in data.get("web", {}).get("results", [])[:max_results]:
                items.append({
                    "title": it.get("title") or "",
                    "url": it.get("url") or "",
                    "content": it.get("description") or "",
                })
            return [x for x in items if x["url"]]
        except Exception:
            return None

    def _try_bing(self, query: str, max_results: int) -> Optional[List[Dict]]:
        if not self.bing_key:
            return None
        try:
            url = "https://api.bing.microsoft.com/v7.0/search"
            headers = {"Ocp-Apim-Subscription-Key": self.bing_key}
            params = {"q": query, "count": max_results, "responseFilter": "Webpages"}
            r = requests.get(url, headers=headers, params=params, timeout=self.timeout)
            if r.status_code != 200:
                return None
            data = r.json()
            items = []
            for it in data.get("webPages", {}).get("value", [])[:max_results]:
                items.append({
                    "title": it.get("name") or "",
                    "url": it.get("url") or "",
                    "content": it.get("snippet") or "",
                })
            return [x for x in items if x["url"]]
        except Exception:
            return None

    # -------- å…¬é–‹ï¼šæ¤œç´¢ --------
    def search(self, query: str, max_results: int = 20) -> List[Dict]:
        # 1) Ollama SDK
        res = self._try_ollama_sdk(query, max_results)
        if res:
            return res[:max_results]

        # 2) Ollama HTTP
        res = self._try_ollama_http(query, max_results)
        if res:
            return res[:max_results]

        # # 3) ç„¡éµRSSï¼ˆã¾ãšGoogle Newsã€ãƒ€ãƒ¡ãªã‚‰Bing Newsï¼‰
        # res = self._try_google_news_rss(query, max_results)
        # if res:
        #     return res[:max_results]
        # res = self._try_bing_news_rss(query, max_results)
        # if res:
        #     return res[:max_results]

        # # 4) ä»»æ„ã®éµãŒã‚ã‚Œã°å¤–éƒ¨API
        # for fn in (self._try_serper, self._try_brave, self._try_bing):
        #     res = fn(query, max_results)
        #     if res:
        #         return res[:max_results]

        # ã™ã¹ã¦å¤±æ•—
        return []

    # -------- æ­£è¦åŒ– --------
    @staticmethod
    def _normalize_search(data) -> List[Dict]:
        if isinstance(data, list):
            items = data
        elif isinstance(data, dict):
            if isinstance(data.get("results"), list):
                items = data["results"]
            elif isinstance(data.get("data"), list):
                items = data["data"]
            else:
                items = next((v for v in data.values() if isinstance(v, list)), [])
        else:
            items = []
        norm = []
        for it in items:
            if not isinstance(it, dict):
                continue
            url = it.get("url") or it.get("link") or ""
            title = it.get("title") or it.get("name") or ""
            content = it.get("content") or it.get("snippet") or it.get("text") or ""
            if url:
                norm.append({"url": url, "title": title, "content": content})
        return norm

# ==============================
# è¨­å®šï¼ˆAPIã‚­ãƒ¼ï¼‰
# ==============================
OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY", "").strip() or "ã“ã“ã«Ollama apiã‚’è¼‰ã›ã¦ãã ã•ã„"
os.environ["OLLAMA_API_KEY"] = OLLAMA_API_KEY

# æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆç’°å¢ƒã«å¿œã˜ã¦è‡ªå‹•é¸æŠï¼‰
search_client = UniversalSearchClient(api_key=OLLAMA_API_KEY)

# ãƒ­ãƒ¼ã‚«ãƒ« LLMï¼ˆgemma3:4bï¼‰
try:
    ollama_local_client = ollama.Client()
except Exception as e:
    st.error(f"Ollama ãƒ­ãƒ¼ã‚«ãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

# ==============================
# æ¤œç´¢ï¼ˆfetchã¯ã—ãªã„ï¼‰
# ==============================
@st.cache_data(ttl=1800)
def web_search_and_fetch(query: str, max_results: int = 20) -> List[Dict]:
    """
    fetchã¯è¡Œã‚ãšã€æ¤œç´¢çµæœï¼ˆtitle/url/snippetï¼‰ã ã‘ã‚’è¿”ã™ã€‚
    RSSãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«ã‚ˆã‚Šã€éµãªã—ã§ã‚‚>0ä»¶ã‚’è¿”ã™ã“ã¨ã‚’ç›®æ¨™ã«å®Ÿè£…ã€‚
    """
    results = search_client.search(query, max_results=max_results)

    status = st.status("æ¤œç´¢çµæœã‚’æ•´å½¢ä¸­...", expanded=False)
    out: List[Dict] = []
    seen = set()
    for i, r in enumerate(results, 1):
        url = r.get("url")
        if not url or url in seen:
            continue
        seen.add(url)
        title = r.get("title") or ""
        content = r.get("content") or ""
        out.append({"title": title, "url": url, "content": content})
        time.sleep(0.01)
    status.update(label=f"âœ… {len(out)}ä»¶ã®æ¤œç´¢çµæœã‚’å–å¾—", state="complete")
    return out

# ==============================
# ãƒŠãƒ©ãƒ†ã‚£ãƒ–ç”Ÿæˆï¼ˆgemma3:4bï¼‰
# ==============================
def analyze_ai_news_narrative(sources: List[Dict], selected_date: Optional[str]) -> str:
    def clip(txt: str, max_chars=8000):
        return txt[:max_chars] if isinstance(txt, str) else ""

    packed_sources = []
    for i, s in enumerate(sources, 1):
        packed_sources.append(
            f"[ã‚½ãƒ¼ã‚¹{i}]\nã‚¿ã‚¤ãƒˆãƒ«: {s.get('title','')}\nURL: {s.get('url','')}\nå†…å®¹:\n{clip(s.get('content',''))}\n"
        )

    system = textwrap.dedent("""
    ã‚ãªãŸã¯çµŒé¨“è±Šå¯ŒãªAIæŠ€è¡“ã‚¸ãƒ£ãƒ¼ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
    æä¾›ã•ã‚ŒãŸæƒ…å ±æºã‚’æ·±ãåˆ†æã—ã€èª­è€…ã«åˆ†ã‹ã‚Šã‚„ã™ãé­…åŠ›çš„ãªè¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    
    é‡è¦ãªè¦ä»¶:
    - å„AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦ã€ç‹¬ç«‹ã—ãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹
    - å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¯æ˜ç¢ºãªã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ã¨è©³ç´°ãªã€Œå†…å®¹ã€ã‚’å«ã‚ã‚‹
    - ç®‡æ¡æ›¸ãã¯ä½¿ç”¨ã›ãšã€æµã‚Œã‚‹ã‚ˆã†ãªãƒŠãƒ©ãƒ†ã‚£ãƒ–ï¼ˆç‰©èªçš„ï¼‰ãªæ–‡ç« ã§æ›¸ã
    - æŠ€è¡“çš„ãªå†…å®¹ã‚’ä¸€èˆ¬èª­è€…ã«ã‚‚ç†è§£ã§ãã‚‹ã‚ˆã†å™›ã¿ç •ã„ã¦èª¬æ˜ã™ã‚‹
    - å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®èƒŒæ™¯ã€æ„ç¾©ã€å½±éŸ¿ã‚’æ·±ãæ˜ã‚Šä¸‹ã’ã‚‹
    - å°‚é–€ç”¨èªã«ã¯ç°¡å˜ãªèª¬æ˜ã‚’æ·»ãˆã‚‹
    - å¼•ç”¨å…ƒã¯æ–‡ä¸­ã«è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã‚€ï¼ˆä¾‹: ã€Œã€œã«ã‚ˆã‚‹ã¨[1]ã€ï¼‰
    - æœ€å¾Œã«ã€Œå¼•ç”¨å…ƒä¸€è¦§ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ã€[ç•ªå·] URL ã®å½¢å¼ã§åˆ—æŒ™
    
    æ–‡ç« ã‚¹ã‚¿ã‚¤ãƒ«:
    - èª­è€…ã‚’å¼•ãè¾¼ã‚€å°å…¥æ–‡ã‹ã‚‰å§‹ã‚ã‚‹
    - ã€Œã§ã‚ã‚‹ã€èª¿ã®è½ã¡ç€ã„ãŸæ–‡ä½“
    - å…·ä½“ä¾‹ã‚„æ¯”å–©ã‚’ç”¨ã„ã¦ç†è§£ã‚’ä¿ƒé€²
    - å„æ®µè½ã¯3-5æ–‡ç¨‹åº¦ã§æ§‹æˆ
    - ãƒ‹ãƒ¥ãƒ¼ã‚¹é–“ã®é–¢é€£æ€§ãŒã‚ã‚Œã°è¨€åŠã™ã‚‹
    - æ—¥æœ¬èªã§å‡ºåŠ›
    """).strip()

    separator = "\n\n" + "="*50 + "\n\n"
    sources_text = separator.join(packed_sources)
    today = selected_date or datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

    prompt = f"""
æœ¬æ—¥ï¼ˆ{today}ï¼‰ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä»¥ä¸‹ã®æƒ…å ±æºã‹ã‚‰æ·±ãåˆ†æã—ã¦æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ã€æƒ…å ±æºã€‘
{sources_text}

ã€å‡ºåŠ›å½¢å¼ã€‘
# æœ¬æ—¥ã®AIæŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ - {today}

[å°å…¥éƒ¨: ä»Šæ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹å…¨ä½“ã®æ¦‚è¦ã‚’2-3æ®µè½ã§]

## [ãƒ‹ãƒ¥ãƒ¼ã‚¹1ã®ã‚¿ã‚¤ãƒˆãƒ«]

[ãƒŠãƒ©ãƒ†ã‚£ãƒ–å½¢å¼ã®è©³ç´°ãªå†…å®¹ã€‚3-5æ®µè½ç¨‹åº¦]

## [ãƒ‹ãƒ¥ãƒ¼ã‚¹2ã®ã‚¿ã‚¤ãƒˆãƒ«]

[ãƒŠãƒ©ãƒ†ã‚£ãƒ–å½¢å¼ã®è©³ç´°ãªå†…å®¹ã€‚3-5æ®µè½ç¨‹åº¦]

[ä»¥é™ã€é‡è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã”ã¨ã«åŒæ§˜ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ]

---

## å¼•ç”¨å…ƒä¸€è¦§
[1] [URL]
[2] [URL]
...
"""
    full_prompt = f"{system}\n\n{prompt}"

    try:
        response = ollama_local_client.chat(
            model='gemma3:4b',
            messages=[{'role': 'user', 'content': full_prompt}],
            options={'temperature': 0.8, 'num_predict': 8000}
        )
        if isinstance(response, dict):
            content = response.get('message', {}).get('content', '')
        else:
            content = getattr(getattr(response, 'message', None), 'content', '') or ''
        return content or "(å¿œç­”ãªã—)"
    except Exception as e:
        error_msg = f"Error during analysis: {str(e)}\n\n"
        error_msg += "Please ensure Ollama is running locally and gemma3:4b model is installed.\n"
        error_msg += "Run: ollama pull gemma3:4b"
        return error_msg

# ==============================
# UIï¼ˆå…¥åŠ›ã‚¯ã‚¨ãƒªã ã‘ãƒ¡ã‚¤ãƒ³å´ã®â€œã§ã£ã‹ã„ãƒãƒ£ãƒƒãƒˆçª“â€ã«ç§»å‹•ï¼‰
# ==============================
st.markdown('<div class="main-header">AI News Daily</div>', unsafe_allow_html=True)
st.markdown('<div class="subtitle">æœ€æ–°ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã€gemma3:4bã§ç‰©èªèª¿ã«æ·±æ˜ã‚Šè§£èª¬</div>', unsafe_allow_html=True)

# --- è¦‹å‡ºã—ç›´ä¸‹ã«å¤§ããªå…¥åŠ›çª“ã‚’é…ç½®ï¼ˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å¤‰æ›´ã—ãªã„ï¼‰ ---
default_query = f"AI news artificial intelligence latest {datetime.now().strftime('%Y-%m-%d')}"
if 'current_query' not in st.session_state:
    st.session_state.current_query = ""

with st.container():
    # å¤§ããè¦‹ã‚„ã™ã„å…¥åŠ›æ¬„ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã¯ãªããƒ¡ã‚¤ãƒ³ï¼‰
    st.markdown('<div class="big-chat">', unsafe_allow_html=True)
    main_query = st.text_area(
        label="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›",
        value=st.session_state.current_query or default_query,
        placeholder="ä¾‹: latest AI research breakthroughs today",
        height=170,
    )
    st.markdown('</div>', unsafe_allow_html=True)
    # å…¥åŠ›ã®å€¤ã ã‘æ›´æ–°ï¼ˆå®Ÿè¡Œã¯å¾“æ¥ã©ãŠã‚Šã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒœã‚¿ãƒ³ã§è¡Œã†ï¼‰
    st.session_state.current_query = (main_query or "").strip()

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆã‚¯ã‚¨ãƒªä»¥å¤–ã¯ãã®ã¾ã¾ï¼‰
with st.sidebar:
    st.header("æ¤œç´¢è¨­å®š")
    # â€» ã‚¯ã‚¨ãƒªå…¥åŠ›ã¯ã“ã“ã«ã¯ç½®ã‹ãªã„ï¼ˆå ´æ‰€ç§»å‹•ã®ã¿ï¼‰
    max_results = st.slider("æ¤œç´¢ä»¶æ•°ï¼ˆä¸Šé™ï¼‰", min_value=5, max_value=50, value=20, step=5)
    selected_date = st.date_input("è¨˜äº‹ã®æ—¥ä»˜ï¼ˆè¦‹å‡ºã—ç”¨ï¼‰", value=datetime.now().date(), format="YYYY/MM/DD")
    selected_date = selected_date.strftime("%Yå¹´%mæœˆ%dæ—¥") if selected_date else None
    st.markdown("---")
    run_btn = st.button("ğŸ” ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ â†’ ğŸ§  ãƒŠãƒ©ãƒ†ã‚£ãƒ–ç”Ÿæˆ", use_container_width=True)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ï¼ˆãã®ã¾ã¾ï¼‰
if 'sources' not in st.session_state: st.session_state.sources = []
if 'article' not in st.session_state: st.session_state.article = ""
if 'current_date' not in st.session_state: st.session_state.current_date = None

# æ—¢å­˜ã®ã€Œå®Ÿè¡Œãƒœã‚¿ãƒ³ã€å‹•ä½œã¯ç¶­æŒ
if run_btn:
    st.session_state.article = ""
    st.session_state.sources = []
    st.session_state.current_date = selected_date
    effective_query = st.session_state.current_query or default_query

    with st.spinner("ğŸ” Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­..."):
        sources = web_search_and_fetch(effective_query, max_results=max_results)
        st.session_state.sources = sources

    if sources:
        st.success(f"âœ… {len(sources)}ä»¶ã®æ¤œç´¢çµæœã‚’å–å¾—ã—ã¾ã—ãŸ")
        with st.spinner("ğŸ¤– ãƒ­ãƒ¼ã‚«ãƒ«ã®gemma3:4bã§åˆ†æãƒ»è¦ç´„ã‚’ç”Ÿæˆä¸­..."):
            article = analyze_ai_news_narrative(sources, selected_date)
            if article:
                st.session_state.article = article
                st.balloons()
    else:
        st.warning("åˆ†æå¯¾è±¡ã¨ãªã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# å‡ºåŠ›è¡¨ç¤ºï¼ˆãã®ã¾ã¾ï¼‰
if st.session_state.article:
    st.markdown("---")
    st.markdown(st.session_state.article, unsafe_allow_html=True)
    with st.expander("å‚ç…§ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ä¸€è¦§ã‚’è¦‹ã‚‹"):
        for i, s in enumerate(st.session_state.sources, 1):
            st.markdown(f"**{i}. {s.get('title','(no title)')}**")
            st.markdown(f"<{s.get('url','')}>", unsafe_allow_html=True)

st.markdown('<div class="footer">Powered by Ollama Web Search + gemma3:4b</div>', unsafe_allow_html=True)